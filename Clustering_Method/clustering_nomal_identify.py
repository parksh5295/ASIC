# Identify nomal clusters and anomalous clusters with nomal data
# Input 'data' is initial data

import numpy as np
import multiprocessing
import os
# from utils.class_row import nomal_class_data

# Helper function for parallel processing of a single cluster
def _process_single_cluster_label(cluster_id, data_features_for_clustering, clusters_assigned, known_normal_samples_features_to_compare, threshold):
    cluster_mask = (clusters_assigned == cluster_id)
    
    if not np.any(cluster_mask):
        return cluster_id, None, -1 # cluster_id, mask, label (-1 indicates no label assigned / empty cluster)

    current_cluster_features = data_features_for_clustering[cluster_mask]

    if current_cluster_features.size == 0 or (known_normal_samples_features_to_compare is not None and known_normal_samples_features_to_compare.size == 0):
        num_normal_in_cluster = 0
    elif known_normal_samples_features_to_compare is None: # Should not happen if fallback logic for global_known_normal_samples_pca is robust
        num_normal_in_cluster = 0
    else:
        try:
            comparison_matrix = (current_cluster_features[:, None, :] == known_normal_samples_features_to_compare[None, :, :])
            all_features_match = np.all(comparison_matrix, axis=2)
            any_known_normal_matches = np.any(all_features_match, axis=1)
            num_normal_in_cluster = np.sum(any_known_normal_matches)
        except ValueError as e:
            num_normal_in_cluster = 0 # Treat as anomalous on error
        except MemoryError as me:
            num_normal_in_cluster = 0 # Treat as anomalous on error
    
    normal_ratio = num_normal_in_cluster / len(current_cluster_features) if len(current_cluster_features) > 0 else 0
    label_for_final_output = 0 if normal_ratio >= threshold else 1
    
    return cluster_id, cluster_mask, label_for_final_output

# Change function signature:
# data_features_for_clustering: NumPy array of features used for clustering (e.g. X_reduced, shape (N, num_pca_features))
# original_labels_aligned: Original labels for each row in data_features_for_clustering (0 or 1, shape (N,))
# clusters_assigned: Cluster IDs assigned to each row in data_features_for_clustering (shape (N,))
# num_total_clusters: Total number of clusters generated by the clustering algorithm
# global_known_normal_samples_pca: Pre-sampled (e.g., 80% of all known normals) PCA features of known normal samples from the entire dataset.
def clustering_nomal_identify(data_features_for_clustering, original_labels_aligned, clusters_assigned, num_total_clusters, global_known_normal_samples_pca=None, threshold_value=0.3, num_processes_for_algo=None):
    
    if global_known_normal_samples_pca is None:
        if original_labels_aligned is not None and data_features_for_clustering.shape[0] == original_labels_aligned.shape[0]:
            temp_known_normal_samples = data_features_for_clustering[original_labels_aligned == 0]
            num_temp_known_normal = temp_known_normal_samples.shape[0]
            if num_temp_known_normal > 1:
                sample_size = int(num_temp_known_normal * 0.95)
                if sample_size == 0 and num_temp_known_normal > 0: sample_size = 1
                if sample_size > num_temp_known_normal : sample_size = num_temp_known_normal
                if sample_size > 0:
                    random_indices = np.random.choice(num_temp_known_normal, size=sample_size, replace=False)
                    global_known_normal_samples_pca = temp_known_normal_samples[random_indices]
                else:
                    global_known_normal_samples_pca = np.array([])
            elif num_temp_known_normal == 1:
                global_known_normal_samples_pca = temp_known_normal_samples
            else:
                global_known_normal_samples_pca = np.array([])
        else:
            raise ValueError("CNI: global_known_normal_samples_pca must be provided, or fallback from original_labels_aligned failed.")

    known_normal_samples_features_to_compare = global_known_normal_samples_pca

    if not (known_normal_samples_features_to_compare is not None and known_normal_samples_features_to_compare.ndim == 2 and known_normal_samples_features_to_compare.shape[0] > 0):
        if data_features_for_clustering.ndim == 2 and data_features_for_clustering.shape[1] > 0:
            known_normal_samples_features_to_compare = np.empty((0, data_features_for_clustering.shape[1]))
        else:
            known_normal_samples_features_to_compare = np.array([])

    final_labels = np.zeros(len(data_features_for_clustering), dtype=int)
    tasks = [(cid, data_features_for_clustering, clusters_assigned, known_normal_samples_features_to_compare, threshold_value) for cid in np.unique(clusters_assigned) if cid != -1] # Process only existing cluster IDs, exclude noise (-1)
    
    actual_clusters_to_process = len(tasks)
    if actual_clusters_to_process == 0:
        return final_labels # Return all zeros if no actual clusters

    if num_processes_for_algo is not None and num_processes_for_algo > 0:
        num_processes_to_use = min(num_processes_for_algo, actual_clusters_to_process) # Don't use more processes than tasks
    else:
        available_cpus = os.cpu_count()
        if available_cpus is None:
            num_processes_to_use = 1 # Fallback to 1 if CPU count cannot be determined
        elif num_processes_for_algo == 0:
            num_processes_to_use = available_cpus
        else: # num_processes_for_algo is None or invalid negative
            num_processes_to_use = max(1, int(available_cpus / 2))
        num_processes_to_use = min(num_processes_to_use, actual_clusters_to_process) # Ensure not more processes than tasks
        if num_processes_to_use == 0 and actual_clusters_to_process > 0 : num_processes_to_use = 1 # Ensure at least 1 process if there are tasks
    
    try:
        if actual_clusters_to_process > 0 and num_processes_to_use > 0:
            with multiprocessing.Pool(processes=num_processes_to_use) as pool:
                results = pool.starmap(_process_single_cluster_label, tasks)
            
            for cluster_id, cluster_mask, label_for_final_output in results:
                if cluster_mask is not None and label_for_final_output != -1:
                    final_labels[cluster_mask] = label_for_final_output
        else:
            for task_args in tasks:
                _ , cluster_mask_seq, label_for_final_output_seq = _process_single_cluster_label(*task_args)
                if cluster_mask_seq is not None and label_for_final_output_seq != -1:
                    final_labels[cluster_mask_seq] = label_for_final_output_seq
    except Exception as e:
        print(f"[ERROR CNI] Processing failed: {e}. Falling back to sequential processing for remaining tasks or all tasks.")
        for task_args in tasks:
            _ , cluster_mask_seq, label_for_final_output_seq = _process_single_cluster_label(*task_args)
            if cluster_mask_seq is not None and label_for_final_output_seq != -1:
                final_labels[cluster_mask_seq] = label_for_final_output_seq

    return final_labels